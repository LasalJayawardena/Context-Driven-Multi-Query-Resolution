





import json
import glob
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io
import base64
from IPython.display import display, HTML
from collections import Counter


import math
import statistics
import pandas as pd
from scipy.stats import entropy as sp_entropy


import warnings
warnings.filterwarnings('ignore')





def process_files(file_pattern="*.json"):
    # Dictionaries to accumulate data for each question
    explanations = {}  # key: question, value: list of explanation strings
    scores = {}        # key: question, value: list of reasonability scores (floats)

    # Find all files that match the given pattern
    file_paths = glob.glob(file_pattern)
    print(f"Found {len(file_paths)} file(s) matching pattern '{file_pattern}'")

    for file_path in file_paths:
        with open(file_path, "r") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                print(f"Error reading {file_path}: {e}")
                continue

        # Each top-level key represents a chat history
        for chat_key, chat_data in data.items():
            if isinstance(chat_data, dict):
                for question, details in chat_data.items():
                    # Skip if details is empty or not a dict (some entries might be empty)
                    if not isinstance(details, dict) or not details:
                        continue

                    # Check if both expected keys exist
                    if "reasonability_score" in details and "explanation" in details:
                        try:
                            score = float(details["reasonability_score"])
                        except ValueError:
                            print(f"Skipping score for question: {question} in {file_path} (invalid number)")
                            continue
                        explanation = details["explanation"]

                        # Append explanation text for the question
                        explanations.setdefault(question, []).append(explanation)
                        # Append score for the question
                        scores.setdefault(question, []).append(score)

    return explanations, scores

def compute_stats(scores_dict):
    stats = {}
    for question, score_list in scores_dict.items():
        arr = np.array(score_list)
        # If only one score, variance and std are set to 0 (ddof=1 requires at least two samples)
        avg = np.mean(arr)
        var = np.var(arr, ddof=1) if len(arr) > 1 else 0.0
        std = np.std(arr, ddof=1) if len(arr) > 1 else 0.0
        q1 = np.percentile(arr, 25)
        q3 = np.percentile(arr, 75)
        iqr = q3 - q1
        stats[question] = {
            "average": avg,
            "variance": var,
            "std_dev": std,
            "iqr": iqr,
            "min": np.min(arr),
            "max": np.max(arr),
            "count": len(arr)
        }
    return stats


explanations, scores = process_files("./assets/llm_reasonability_scores/*.json")
    
stats = compute_stats(scores)

# Print the computed stats for each question
for question, stat in stats.items():
    print(f"Question: {question}")
    for key, value in stat.items():
        print(f"  {key}: {value}")
    print("-" * 40)


# Optionally, save the results to JSON files for further analysis
with open("./assets/llm_reasonability_stats/explanations_output.json", "w") as ef:
    json.dump(explanations, ef, indent=4)
with open("./assets/llm_reasonability_stats/scores_output.json", "w") as sf:
    json.dump(scores, sf, indent=4)
with open("./assets/llm_reasonability_stats/stats_output.json", "w") as stf:
    json.dump(stats, stf, indent=4)





def process_files_with_model_id(file_pattern="./final_clean_llm_assets/*.json"):
    """
    Process all JSON files matching the given pattern.
    For each file, extract the model ID from the filename (by taking the basename and using [:-24]).
    For each question in the file, store a record with:
      - 'model_id': extracted from filename
      - 'score': reasonability score (float)
      - 'explanation': explanation text (str)
      
    Returns:
        question_data (dict): keys are questions, values are lists of records.
    """
    question_data = {}  # key: question, value: list of records
    
    file_paths = sorted(glob.glob(file_pattern))
    print(f"Found {len(file_paths)} file(s) matching pattern '{file_pattern}'")
    
    for file_path in file_paths:
        # Extract model_id from file name
        model_id = os.path.basename(file_path)[:-24]
        with open(file_path, "r") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                print(f"Error reading {file_path}: {e}")
                continue
        
        # Each top-level key represents a chat history
        for chat_key, chat_data in data.items():
            if isinstance(chat_data, dict):
                for question, details in chat_data.items():
                    # Skip if details is empty or not a dict (some entries might be empty)
                    if not isinstance(details, dict) or not details:
                        continue
                    # Check if both expected keys exist
                    if "reasonability_score" in details and "explanation" in details:
                        try:
                            score = float(details["reasonability_score"])
                        except ValueError:
                            print(f"Skipping score for question: {question} in {file_path} (invalid number)")
                            continue
                        explanation = details["explanation"]
                        
                        record = {
                            "model_id": model_id,
                            "score": score,
                            "explanation": explanation
                        }
                        question_data.setdefault(question, []).append(record)
    
    # Save the raw question data (with model ids, scores and explanations)
    with open("./assets/llm_reasonability_advanced_stats/question_data_output.json", "w") as qf:
        json.dump(question_data, qf, indent=4, sort_keys=True)
    print("Saved question_data_output.json")
    
    return question_data

def compute_confidence_and_impartial_models(question_data):
    """
    For each question in question_data, compute statistics:
      - average, variance, std deviation, median, count.
      - 95% confidence interval: avg ± 1.96*(std/√n).
      
    We use the median to identify 'impartial models' because the median is robust to outliers,
    whereas the average can be skewed by extreme values.
    
    Also aggregates overall impartial model counts to determine:
      - Most impartial judge(s) (model_id(s) with highest frequency)
      - Top 5 impartial models overall (sorted by frequency)
      
    Returns:
        results (dict): Contains:
          - 'per_question_stats': dict keyed by question with stats and impartial models list.
          - 'overall_impartial': dict with keys:
                'model_frequency': {model_id: count, ...},
                'most_impartial_judges': list of model_id(s),
                'top_5_impartial_models': list of model_ids.
    """
    per_question_stats = {}
    overall_impartial_counts = {}

    for question in sorted(question_data.keys()):
        records = question_data[question]
        scores = np.array([rec["score"] for rec in records])
        n = len(scores)
        avg = np.mean(scores)
        var = np.var(scores, ddof=1) if n > 1 else 0.0
        std = np.std(scores, ddof=1) if n > 1 else 0.0
        median = np.median(scores)
        
        # Calculate standard error and 95% confidence margin
        if n > 1 and std > 0:
            se = std / np.sqrt(n)
            ci_margin = 1.96 * se
        else:
            ci_margin = 0.0
        
        # Identify impartial models: those with scores closest to the median
        diffs = np.abs(scores - median)
        min_diff = np.min(diffs)
        impartial_model_ids = [rec["model_id"] for rec, diff in zip(records, diffs) if np.isclose(diff, min_diff, atol=1e-6)]
        
        # Update overall impartial counts
        for mid in impartial_model_ids:
            overall_impartial_counts[mid] = overall_impartial_counts.get(mid, 0) + 1
        
        per_question_stats[question] = {
            "average": avg,
            "variance": var,
            "std_dev": std,
            "median": median,
            "count": n,
            "95%_CI_margin": ci_margin,  # This is the ± value
            "impartial_models": sorted(impartial_model_ids)
        }
    
    # Determine overall impartial models:
    if overall_impartial_counts:
        max_freq = max(overall_impartial_counts.values())
        most_impartial_judges = sorted([mid for mid, count in overall_impartial_counts.items() if count == max_freq])
        # Get top 5 impartial models by frequency (sorted descending by count)
        sorted_models = sorted(overall_impartial_counts.items(), key=lambda x: x[1], reverse=True)
        top_5_impartial_models = [mid for mid, count in sorted_models[:5]]
    else:
        most_impartial_judges = []
        top_5_impartial_models = []
    
    overall_impartial = {
        "model_frequency": overall_impartial_counts,
        "most_impartial_judges": most_impartial_judges,
        "top_5_impartial_models": top_5_impartial_models
    }
    
    # Save stats and overall impartial results to files.
    with open("./assets/llm_reasonability_advanced_stats/stats_output.json", "w") as stf:
        json.dump(per_question_stats, stf, indent=4, sort_keys=True)
    with open("./assets/llm_reasonability_advanced_stats/impartial_output.json", "w") as imf:
        json.dump(overall_impartial, imf, indent=4, sort_keys=True)
    
    print("Saved stats_output.json and impartial_output.json")
    
    results = {
        "per_question_stats": per_question_stats,
        "overall_impartial": overall_impartial
    }
    
    return results

def display_question_stats(question_data, per_question_stats):
    """
    Displays a table (using pandas DataFrame) for each question with summary statistics:
      - Question, Avg Score, 95% CI (displayed as "avg ± margin"), Median, Std Dev, Count, Impartial Models,
        and a plot showing the normal distribution fit for the scores.
      
    For the plot column, a small matplotlib figure is generated for each question.
    The plot shows a smooth normal distribution curve using the computed average and std_dev.
    (If std_dev is zero, it will simply mark the score.)
    
    This function is designed for a Jupyter Notebook.
    """
    table_rows = []
    
    for question in sorted(question_data.keys()):
        records = question_data[question]
        scores = np.array([rec["score"] for rec in records])
        stats = per_question_stats.get(question, {})
        n = stats.get("count", len(scores))
        avg = stats.get("average", np.mean(scores))
        std = stats.get("std_dev", np.std(scores, ddof=1) if len(scores) > 1 else 0.0)
        median = stats.get("median", np.median(scores))
        ci_margin = stats.get("95%_CI_margin", 0.0)
        
        # Create a normal distribution plot based on avg and std.
        fig, ax = plt.subplots(figsize=(3, 1.5))
        if std > 0:
            x = np.linspace(avg - 4*std, avg + 4*std, 100)
            y = (1/(std * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x-avg)/std)**2)
            ax.plot(x, y, lw=2)
        else:
            # For a single score or zero std, just mark the average.
            ax.axvline(avg, color="red", lw=2)
        ax.set_title("Normal Dist", fontsize=8)
        ax.set_xticks([])
        ax.set_ylabel("")
        plt.tight_layout()
        
        # Save the figure to a PNG in memory and encode as base64.
        buf = io.BytesIO()
        plt.savefig(buf, format="png", dpi=100, bbox_inches="tight")
        buf.seek(0)
        img_base64 = base64.b64encode(buf.read()).decode("utf-8")
        plt.close(fig)
        img_html = f'<img src="data:image/png;base64,{img_base64}" style="display: block;">'
        
        # Build a row for the table.
        row = {
            "Question": question,
            "Avg Score": round(avg, 2),
            "95% CI": f"± {round(ci_margin, 2)}",
            "Median": round(median, 2),
            "Std Dev": round(std, 2),
            "Count": n,
            "Impartial Models": ", ".join(stats.get("impartial_models", [])),
            "Plot": img_html
        }
        table_rows.append(row)
    
    df = pd.DataFrame(table_rows)
    pd.set_option('display.max_colwidth', None)
    html_table = df.to_html(escape=False, index=False)
    display(HTML(html_table))
    
    # Optionally, save this table to an HTML file.
    with open("./assets/llm_reasonability_advanced_stats/question_stats_table.html", "w") as html_file:
        html_file.write(html_table)
    print("Saved question_stats_table.html")


# 1. Process files and build question_data with model IDs.
question_data = process_files_with_model_id("./assets/llm_reasonability_scores/*.json")

# 2. Compute stats, confidence intervals, and identify impartial models.
results = compute_confidence_and_impartial_models(question_data)
per_question_stats = results["per_question_stats"]
overall_impartial = results["overall_impartial"]

# Print overall impartial models results.
print("Overall Impartial Models Summary:")
print("Model Frequency:", overall_impartial["model_frequency"])
print("Most Impartial Judge(s):", overall_impartial["most_impartial_judges"])
print("Top 5 Impartial Models:", overall_impartial["top_5_impartial_models"])

# 3. Display the question statistics in a table with a normal distribution plot (best viewed in a Jupyter Notebook)
display_question_stats(question_data, per_question_stats)





def plot_score_frequency_for_each_question(question_data):
    """
    For each question in the provided question_data (a dictionary where each key is a question
    and its value is a list of records with a 'score'), plot a histogram of the reasonability scores.
    
    The x-axis represents the score (e.g., 0 to 10) and the y-axis represents the frequency.
    The function displays each graph one after the other.
    
    Args:
        question_data (dict): Dictionary where each key is a question and the value is a list of records.
                              Each record should include a 'score' key (a float).
    """
    # Loop over questions in sorted order for consistency
    for question in sorted(question_data.keys()):
        # Extract scores for this question
        scores = [record['score'] for record in question_data[question]]
        
        # Create a new figure for each question
        plt.figure(figsize=(6, 4))
        plt.hist(scores, bins=10, edgecolor="black", alpha=0.7, range=[0, 10])
        plt.title(f"Score Frequency for:\n{question}", fontsize=10)
        plt.xlabel("Reasonability Score")
        plt.ylabel("Frequency")
        plt.grid(True, linestyle='--', alpha=0.5)
        plt.tight_layout()
        plt.show()


question_data = process_files_with_model_id("./assets/llm_reasonability_scores/*.json")
plot_score_frequency_for_each_question(question_data)





def analyze_models(model_info, question_data):
    """
    Merge the question_data (which contains model_id, score, explanation per question)
    with the provided model_info dictionary (formatted as shown in your JSON).
    
    Then:
      - Calculate stats for open vs. closed source models.
      - Calculate stats for reasoning vs. non-reasoning models.
      - Compute the correlation between parameter count and reasonability score.
      - Identify extreme parameter models (lowest 10% and highest 10% by parameter count).
      - Plot a histogram of reasonability scores, highlighting the low and high parameter groups
        in different colors.
        
    Args:
        model_info (dict): Model information dictionary.
        question_data (dict): Dictionary mapping questions to a list of records 
                              (each record with keys 'model_id', 'score', 'explanation').
    
    Returns:
        merged_df (DataFrame): A merged DataFrame containing reasonability records with
                               model info and an added 'param_category' column.
    """
    # Aggregate records from question_data into a list.
    records = []
    for question, rec_list in question_data.items():
        for rec in rec_list:
            rec_copy = rec.copy()
            rec_copy["question"] = question
            records.append(rec_copy)
    df = pd.DataFrame(records)
    
    # Convert the model_info dictionary to a DataFrame.
    model_info_df = pd.DataFrame.from_dict(model_info, orient='index')
    model_info_df.index.name = "model_id"
    model_info_df.reset_index(inplace=True)
    
    # Merge the reasonability records with model information.
    merged_df = pd.merge(df, model_info_df, on="model_id", how="left")
    
    # Convert "parameter count" to numeric.
    # Remove commas and treat non-numeric entries as NaN.
    merged_df["parameter_count"] = pd.to_numeric(
        merged_df["parameter count"].astype(str).str.replace(",", ""),
        errors="coerce"
    )
    
    # Compute statistics for open vs. closed source models.
    open_df = merged_df[merged_df["open / closed source"].str.lower().str.contains("open", na=False)]
    closed_df = merged_df[merged_df["open / closed source"].str.lower().str.contains("closed", na=False)]
    
    print("Stats for Open Source Models:")
    print(open_df["score"].describe())
    print("\nStats for Closed Source Models:")
    print(closed_df["score"].describe())
    
    # Compute statistics for reasoning vs. non-reasoning models.
    reasoning_df = merged_df[merged_df["is a reasoning model?"].str.lower() == "true"]
    non_reasoning_df = merged_df[merged_df["is a reasoning model?"].str.lower() == "false"]
    
    print("\nStats for Reasoning Models:")
    print(reasoning_df["score"].describe())
    print("\nStats for Non-Reasoning Models:")
    print(non_reasoning_df["score"].describe())
    
    # Analyze the effect of parameter count on reasonability score.
    valid_param_df = merged_df.dropna(subset=["parameter_count"])
    if not valid_param_df.empty:
        corr = valid_param_df["parameter_count"].corr(valid_param_df["score"])
        print("\nCorrelation between parameter count and reasonability score:", corr)
        # Identify extreme parameter models: lowest 10% and highest 10%.
        low_threshold = 8000000000
        high_threshold = 30000000000
    else:
        print("\nNo valid parameter count data available.")
        low_threshold = high_threshold = None
    
    # Label each record based on parameter count.
    def label_param(row):
        if pd.isna(row["parameter_count"]):
            # Instead of 'unknown', missing parameter count is now treated as 'closed'
            return "Closed Source"
        elif low_threshold is not None and row["parameter_count"] <= low_threshold:
            return "Small"
        elif high_threshold is not None and row["parameter_count"] >= high_threshold:
            return "Large"
        else:
            return "Mid Size"
    
    merged_df["param_category"] = merged_df.apply(label_param, axis=1)
    
    # Plot the overall distribution of reasonability scores by extreme parameter groups.
    plt.figure(figsize=(8, 6))
    bins = np.linspace(0, 10, 21)  # bins from 0 to 10 in steps of 0.5
    categories = ["Small", "Large", "Mid Size", "Closed Source"]
    colors = {"Small": "blue", "Large": "red", "Mid Size": "gray", "Closed Source": "green"}
    
    for cat in categories:
        cat_data = merged_df[merged_df["param_category"] == cat]["score"]
        if not cat_data.empty:
            plt.hist(cat_data, bins=bins, alpha=0.6, label=f"{cat}", color=colors[cat], edgecolor="black")
    
    plt.xlabel("Reasonability Score")
    plt.ylabel("Frequency")
    plt.title("Distribution of Reasonability Scores by Parameter Category")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.show()
    
    return merged_df


# Load model_info from JSON 
with open("./data/llm_model_info.json", "r") as f:
    model_info = json.load(f)

# Process question data using your file-processing function.
question_data = process_files_with_model_id("./assets/llm_reasonability_scores/*.json")

# Run the analysis to merge model info with question data.
merged_df = analyze_models(model_info, question_data)


def plot_model_category_distributions(merged_df):
    """
    Plot three histograms in one figure:
      1. Reasoning vs. Non-reasoning models.
      2. Open vs. Closed source models.
      3. Parameter groups: Small (<8B), Mid (>=8B), and Closed (missing parameter count).
      
    Each histogram shows the distribution of reasonability scores.
    """
    bins = np.linspace(0, 10, 21)  # Define bins for score distribution
    fig, axs = plt.subplots(2, 1, figsize=(10, 18))
    
    # 1. Reasoning vs. Non-reasoning histogram.
    reasoning = merged_df[merged_df["is a reasoning model?"].str.lower() == "true"]["score"]
    non_reasoning = merged_df[merged_df["is a reasoning model?"].str.lower() == "false"]["score"]
    
    axs[0].hist(reasoning, bins=bins, alpha=0.7, color="blue", label="Reasoning")
    axs[0].hist(non_reasoning, bins=bins, alpha=0.7, color="orange", label="Non-reasoning")
    axs[0].set_xlabel("Reasonability Score")
    axs[0].set_ylabel("Frequency")
    axs[0].set_title("Reasoning vs. Non-Reasoning LLM Models")
    axs[0].legend()
    axs[0].grid(True, linestyle="--", alpha=0.5)
    
    # 2. Open vs. Closed source histogram.
    open_df = merged_df[merged_df["open / closed source"].str.lower().str.contains("open", na=False)]["score"]
    closed_df = merged_df[merged_df["open / closed source"].str.lower().str.contains("closed", na=False)]["score"]
    
    axs[1].hist(open_df, bins=bins, alpha=0.7, color="green", label="Open Source")
    axs[1].hist(closed_df, bins=bins, alpha=0.7, color="red", label="Closed Source")
    axs[1].set_xlabel("Reasonability Score")
    axs[1].set_title("Open vs. Closed Source LLM Models")
    axs[1].legend()
    axs[1].grid(True, linestyle="--", alpha=0.5)
    
    # # 3. Parameter groups: Small (<8B), Mid (>=8B), Closed.
    # def param_group(row):
    #     if pd.isna(row["parameter_count"]):
    #         return "closed"
    #     elif row["parameter_count"] < 8e9:
    #         return "small"
    #     else:
    #         return "mid"
    
    # merged_df["param_group"] = merged_df.apply(param_group, axis=1)
    # small = merged_df[merged_df["param_group"] == "small"]["score"]
    # mid = merged_df[merged_df["param_group"] == "mid"]["score"]
    # closed = merged_df[merged_df["param_group"] == "closed"]["score"]
    
    # axs[2].hist(small, bins=bins, alpha=0.7, color="purple", label="Small (<8B)")
    # axs[2].hist(mid, bins=bins, alpha=0.7, color="cyan", label="Mid (>=8B)")
    # axs[2].hist(closed, bins=bins, alpha=0.7, color="gray", label="Closed")
    # axs[2].set_xlabel("Reasonability Score")
    # axs[2].set_title("Parameter Groups")
    # axs[2].legend()
    # axs[2].grid(True, linestyle="--", alpha=0.5)
    
    plt.tight_layout()
    plt.show()


# Plot the additional graphs:
plot_model_category_distributions(merged_df)


def plot_open_closed_pie(model_info):
    """
    Draw a pie chart showing the percentage of open source vs. closed source models.
    
    Args:
        model_info (dict): Dictionary containing model information. Each key is a model name,
                           and its value is a dict that includes the key "open / closed source".
    """
    open_count = 0
    closed_count = 0
    
    for model, info in model_info.items():
        src = info.get("open / closed source", "").lower()
        if "open" in src:
            open_count += 1
        elif "closed" in src:
            closed_count += 1
    
    sizes = [open_count, closed_count]
    labels = ["Open Source", "Closed Source"]

    plt.figure(figsize=(6,6))
    plt.pie(sizes, labels=labels, autopct="%1.1f%%", startangle=140)
    plt.title("Percentage of Open vs. Closed Source Models")
    plt.axis("equal")  # Ensures the pie is drawn as a circle.
    plt.show()


plot_open_closed_pie(model_info)





def display_html_entries(html_dict):
    """
    Iterates over the given dictionary and displays each HTML snippet.
    
    Args:
        html_dict (dict): A dictionary where keys are identifiers (e.g., interaction IDs and questions)
                          and values are either a string of HTML content or a list of HTML content.
    """
    for key, content in html_dict.items():
        print("\n-----")
        print("Key:", key)
        print("-----")
        # If the content is a string and contains HTML tags, display it.
        if isinstance(content, str) and "<html" in content.lower():
            display(HTML(content))
        # If the content is a list, iterate over each item.
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, str) and "<html" in item.lower():
                    display(HTML(item))
                else:
                    print(item)
        else:
            print(content)


with open("./data/html_context.json", "r") as f:
    html_context_dict = json.load(f)


clean_context_list = []
for x in html_context_dict:
    if "Context" in x:
        clean_context_list.append({x:html_context_dict[x]})
        continue
    else:
        display_html_entries({x:html_context_dict[x]})





# Isee Intent Categories (https://github.com/isee4xai/iSeeOntoAPI)
# ['Actionability',
#      'Compliancy',
#      'Comprehensibility',
#      'Debugging',
#      'Education',
#      'Effectiveness',
#      'Efficiency',
#      'Performance',
#      'Persuasiveness',
#      'Satisfaction',
#      'Scruitability',
#      'Transparency',
#      'Trust']


with open("./data/intents_used_results_with_exp.json", "r") as f:
    chat_post_process_results = json.load(f)


import json
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# --- Extraction Functions  ---

def extract_intents(data):
    """
    For each chat history, extract:
      - The intents from the original questions (listed under "original_question")
      - The intents from the follow-up/clarification questions (all keys other than "original_question")
    
    Returns:
      all_orig: list of lists with original question intents (order preserved)
      all_followups: list of lists with follow-up question intents (order preserved)
    """
    all_orig = []
    all_followups = []
    
    for chat_key, chat in data.items():
        orig_intents = []
        followup_intents = []
        
        # Process the original_question part (an array of one object).
        if "original_question" in chat:
            for q_obj in chat["original_question"]:
                for question, details in q_obj.items():
                    intent = details.get("intent", "").strip()
                    orig_intents.append(intent)
        
        # Process follow-up questions (all keys other than "original_question")
        for key, value in chat.items():
            if key == "original_question":
                continue
            intents = value.get("intent", [])
            if isinstance(intents, list):
                cleaned = [i.strip() for i in intents]
                followup_intents.extend(cleaned)
            elif isinstance(intents, str):
                followup_intents.append(intents.strip())
        
        all_orig.append(orig_intents)
        all_followups.append(followup_intents)
    
    return all_orig, all_followups

def transition_analysis(all_orig, all_followups):
    """
    Compute transitions over the full sequence of intents (original + follow-up).
    
    Returns:
      full_sequence_transitions: Counter for all adjacent transitions in the full sequence.
    """
    full_sequence_transitions = Counter()
    
    for orig, follow in zip(all_orig, all_followups):
        full_sequence = orig + follow
        for i in range(len(full_sequence) - 1):
            full_sequence_transitions[(full_sequence[i], full_sequence[i+1])] += 1
    
    return full_sequence_transitions

def markov_chain_analysis(full_sequence_transitions, possible_intents):
    """
    Build a Markov chain transition matrix (probabilities) based on full sequence transitions.
    """
    # Initialize counts for each possible transition.
    matrix = {intent: {intent2: 0 for intent2 in possible_intents} for intent in possible_intents}
    row_sums = {intent: 0 for intent in possible_intents}
    
    for (current, nxt), count in full_sequence_transitions.items():
        matrix[current][nxt] += count
        row_sums[current] += count
    
    transition_matrix = {intent: {} for intent in possible_intents}
    for intent in possible_intents:
        for intent2 in possible_intents:
            if row_sums[intent] > 0:
                transition_matrix[intent][intent2] = matrix[intent][intent2] / row_sums[intent]
            else:
                transition_matrix[intent][intent2] = 0.0
    return transition_matrix

def print_transition_matrix(transition_matrix):
    """
    Print the transition matrix as a pandas DataFrame.
    """
    df = pd.DataFrame(transition_matrix).T
    print(df)

# --- Plotting Functions ---

def plot_intent_frequency(intents, title="Intent Frequency"):
    """
    Plot a bar chart showing the frequency distribution of intents.
    """
    freq = Counter(intents)
    # Sort the items by frequency descending.
    items = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    labels, counts = zip(*items)
    
    plt.figure(figsize=(10,6))
    plt.bar(labels, counts)
    plt.title(title)
    plt.xlabel("Intent")
    plt.ylabel("Frequency")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

def plot_transition_matrix(transition_matrix, possible_intents):
    """
    Plot a heatmap for the Markov chain transition matrix.
    """
    matrix = np.array([[transition_matrix[row][col] for col in possible_intents] for row in possible_intents])
    
    fig, ax = plt.subplots(figsize=(10, 8))
    cax = ax.imshow(matrix, cmap='viridis', interpolation='nearest')
    plt.title("Full Sequence Markov Chain Transition Matrix")
    plt.xticks(np.arange(len(possible_intents)), possible_intents, rotation=45, ha="right")
    plt.yticks(np.arange(len(possible_intents)), possible_intents)
    fig.colorbar(cax)
    
    for i in range(len(possible_intents)):
        for j in range(len(possible_intents)):
            ax.text(j, i, f"{matrix[i, j]:.2f}", ha="center", va="center", color="w")
    
    plt.tight_layout()
    plt.show()

# --- Ordered Intent Sequence Output Function ---

def print_intent_sequences(all_orig, all_followups):
    """
    For each chat, print the ordered sequence of intents.
    Format:
       intent1 -> intent2 : intent_x1 -> intent_x2
    where the left part is from original questions and the right part is from follow-ups.
    """
    print("\nOrdered Intent Sequences by Chat:")
    for idx, (orig, follow) in enumerate(zip(all_orig, all_followups), 1):
        orig_seq = " -> ".join(orig) if orig else "None"
        follow_seq = " -> ".join(follow) if follow else "None"
        print(f"Chat {idx}: {orig_seq} : {follow_seq}")

# --- Analysis Commentary Function ---

def analyze_full_sequence(full_sequence_transitions):
    """
    Print commentary based solely on the full sequence transitions.
    """
    print("\n--- Full Sequence Analysis ---")
    print("Significant transitions (occurring more than twice) in the full sequence are:")
    for (current, nxt), count in full_sequence_transitions.items():
        if count > 2:
            print(f"  {current} -> {nxt}: {count} times.")
    print("\nObservations:")
    print("  • The full sequence Markov chain shows that 'Transparency' is a major focal point,")
    print("    often transitioning to and from other intents like 'Performance' and 'Debugging'.")
    print("  • This suggests that users initially ask for explanations and then often follow up with")
    print("    questions that either further probe the system's transparency or check for potential issues.")
    print("  • Such patterns may indicate that users find the system's outputs complex and require additional")
    print("    clarifications to fully understand or validate the decisions made by the AI.")


# Define the list of all possible intents.
possible_intents = ['Actionability',
 'Compliancy',
 'Comprehensibility',
 'Debugging',
 'Education',
 'Effectiveness',
 'Efficiency',
 'Performance',
 'Persuasiveness',
 'Satisfaction',
 'Scruitability',
 'Transparency',
 'Trust']

# Extract intent sequences from the data.
all_orig, all_followups = extract_intents(chat_post_process_results)

# Create a flattened list for frequency plots.
all_original_intents = [intent for sublist in all_orig for intent in sublist]
all_followup_intents = [intent for sublist in all_followups for intent in sublist]

# Plot frequency distribution for original and follow-up intents.
plot_intent_frequency(all_original_intents, title="Original Question Intent Frequency")
plot_intent_frequency(all_followup_intents, title="Clarification Question Intent Frequency")

# Compute full sequence transitions and Markov chain.
full_sequence_transitions = transition_analysis(all_orig, all_followups)
transition_matrix = markov_chain_analysis(full_sequence_transitions, possible_intents)

# Plot the full sequence Markov chain transition matrix.
plot_transition_matrix(transition_matrix, possible_intents)

# Print ordered sequences for each chat.
print_intent_sequences(all_orig, all_followups)

# Provide a full sequence-only analysis commentary.
analyze_full_sequence(full_sequence_transitions)



# Transitions from the last original to the first follow-up per chat:
#   Performance -> Transparency: 3 time(s).
#   Actionability -> Education: 1 time(s).
#   Transparency -> Transparency: 7 time(s).
#   Debugging -> Transparency: 4 time(s).
#   Actionability -> Transparency: 2 time(s).
#   Performance -> Performance: 2 time(s).
#   Transparency -> Performance: 1 time(s).
#   Transparency -> Debugging: 1 time(s).
#   Debugging -> Comprehensibility: 3 time(s).
#   Transparency -> Comprehensibility: 1 time(s).

# Significant transitions in the full sequence (occurring more than twice):
#   Transparency -> Performance: 4 times.
#   Performance -> Transparency: 6 times.
#   Transparency -> Transparency: 16 times.
#   Transparency -> Debugging: 7 times.
#   Debugging -> Transparency: 7 times.
#   Debugging -> Comprehensibility: 3 times.


non_zero_intents = ['Actionability',
 'Comprehensibility',
 'Debugging',
 'Education',

 'Performance',
 'Transparency',
 'Trust']

# Plot the full sequence Markov chain transition matrix.
plot_transition_matrix(transition_matrix, non_zero_intents)


def print_intent_sequences_with_id(data):
    """
    For each chat, print the ordered sequence of intents with the chat ID.
    Format:
      Chat {counter} (ID: {chat_id}): intent1 -> intent2 : intent_x1 -> intent_x2
    where the left part is from original questions and the right part is from follow-ups.
    """
    print("\nOrdered Intent Sequences by Chat:")
    for idx, (chat_id, chat) in enumerate(data.items(), 1):
        orig_intents = []
        followup_intents = []
        
        # Process original_question intents.
        if "original_question" in chat:
            for q_obj in chat["original_question"]:
                for question, details in q_obj.items():
                    intent = details.get("intent", "").strip()
                    orig_intents.append(intent if intent else "None")
                    
        # Process follow-up/clarification intents (all keys except "original_question").
        for key, value in chat.items():
            if key == "original_question":
                continue
            intents = value.get("intent", [])
            if isinstance(intents, list):
                followup_intents.extend([i.strip() for i in intents if i.strip()])
            elif isinstance(intents, str):
                followup_intents.append(intents.strip())
        
        orig_seq = " -> ".join(orig_intents) if orig_intents else "None"
        follow_seq = " -> ".join(followup_intents) if followup_intents else "None"
        print(f"Chat {idx} (ID: {chat_id}): {orig_seq} : {follow_seq}")


print_intent_sequences_with_id(data)





def shannon_entropy(data, base=2):
    """
    Calculate the Shannon entropy of a dataset using scipy.stats.entropy.
    
    Parameters:
        data (list): List of elements (e.g., scores or categorical values).
        base (float): The logarithmic base used (default is 2 for bits).
    
    Returns:
        float: The calculated Shannon entropy.
    """
    counts = Counter(data)
    total = sum(counts.values())
    probabilities = [freq / total for freq in counts.values()]
    return sp_entropy(probabilities, base=base)

def simpson_index(data):
    """
    Calculate Simpson's diversity index for a dataset.
    
    Parameters:
        data (list): List of elements.
    
    Returns:
        float: Simpson's diversity index (1 - sum(p_i^2)).
    """
    counts = Counter(data)
    total = sum(counts.values())
    sum_pi2 = sum((freq / total) ** 2 for freq in counts.values())
    return 1 - sum_pi2

def gini_coefficient(data):
    """
    Calculate the Gini coefficient for a list of numerical values.
    
    Parameters:
        data (list of numbers): The dataset for which to calculate inequality.
    
    Returns:
        float: The Gini coefficient, where 0 represents perfect equality.
    """
    if not data:
        return 0
    sorted_data = sorted(data)
    n = len(data)
    cumulative = 0
    for i, x in enumerate(sorted_data, start=1):
        cumulative += i * x
    total = sum(sorted_data)
    return (2 * cumulative) / (n * total) - (n + 1) / n

def variance_metric(data):
    """
    Calculate the variance of numerical data.
    
    Parameters:
        data (list of numbers): List of numerical scores.
    
    Returns:
        float: The variance of the dataset.
    """
    if len(data) < 2:
        return 0
    return statistics.variance(data)

def calculate_distribution_stats(data_dict):
    """
    Calculate distribution statistics for a dictionary of questions.
    
    Parameters:
        data_dict (dict): A dictionary where keys are questions and values are lists of responses.
    
    Returns:
        pandas.DataFrame: A dataframe with each question's distribution stats.
    """
    results = []
    for question, responses in data_dict.items():
        # Compute stats for all responses (categorical metrics)
        entropy_val = shannon_entropy(responses)
        simpson_val = simpson_index(responses)
        
        # Attempt to treat responses as numeric for inequality metrics.
        try:
            numeric_responses = [float(x) for x in responses]
            gini_val = gini_coefficient(numeric_responses)
            variance_val = variance_metric(numeric_responses)
        except ValueError:
            gini_val = None
            variance_val = None
        
        results.append({
            'Question': question,
            'Shannon Entropy': entropy_val,
            'Simpson Index': simpson_val,
            'Gini Coefficient': gini_val,
            'Variance': variance_val
        })
    
    return pd.DataFrame(results)


with open("./assets/llm_reasonability_stats/scores_output.json", "r") as f:
    score_dict = json.load(f)


stats_df = calculate_distribution_stats(score_dict)


stats_df


stats_df.describe()



